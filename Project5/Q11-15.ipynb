{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pytz\n",
    "import numpy as np\n",
    "import json\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import datetime, time\n",
    "import pytz\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting features\n",
    "def extracting_5_agg():\n",
    "    \n",
    "    posting_time = []\n",
    "    num_retweets = []\n",
    "    num_followers = []\n",
    "    pst_tz = pytz.timezone('America/Los_Angeles')\n",
    "    hashtags = ['tweets_#gohawks.txt', 'tweets_#gopatriots.txt', 'tweets_#nfl.txt', 'tweets_#patriots.txt', 'tweets_#sb49.txt', 'tweets_#superbowl.txt']\n",
    "    for hashtag in hashtags:\n",
    "        print (\"Reading from file \", hashtag)\n",
    "        file = open('./data/'+hashtag, encoding = 'utf8')\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            posting_time.append(data['citation_date'])\n",
    "            num_retweets.append(data['metrics']['citations']['total'])\n",
    "            num_followers.append(data['author']['followers'])\n",
    "        file.close()\n",
    "        \n",
    "    hours = int((max(posting_time)-min(posting_time))/3600)+1\n",
    "    tmp = np.zeros([hours, 5])\n",
    "    start_time = min(posting_time)\n",
    "    start_hour = (datetime.datetime.fromtimestamp(start_time, pst_tz)).hour\n",
    "\n",
    "    for i in range(hours):\n",
    "        tmp[i,4] = (start_hour+i)%24\n",
    "    for i in range(len(posting_time)):\n",
    "        tmp[int((posting_time[i]-start_time)/3600), 0] += 1\n",
    "        tmp[int((posting_time[i]-start_time)/3600), 1] += num_retweets[i]\n",
    "        tmp[int((posting_time[i]-start_time)/3600), 2] += num_followers[i]\n",
    "        if tmp[int((posting_time[i]-start_time)/3600), 3] < num_followers[i]:\n",
    "            tmp[int((posting_time[i]-start_time)/3600), 3] = num_followers[i]\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extracting features\n",
    "# def extracting_3(hashtag):\n",
    "#     pst_tz = pytz.timezone('America/Los_Angeles')\n",
    "#     file = open('./data/'+hashtag, encoding = 'utf8')\n",
    "    \n",
    "#     posting_time_1 = []\n",
    "#     num_retweets_1 = []\n",
    "#     num_followers_1 = []\n",
    "    \n",
    "#     posting_time_2 = []\n",
    "#     num_retweets_2 = []\n",
    "#     num_followers_2 = []\n",
    "    \n",
    "#     posting_time_3 = []\n",
    "#     num_retweets_3 = []\n",
    "#     num_followers_3 = []\n",
    "    \n",
    "#     for line in file:\n",
    "#         data = json.loads(line)\n",
    "#         time = data['citation_date']\n",
    "#         if (time < time1):\n",
    "#             posting_time_1.append(data['citation_date'])\n",
    "#             num_retweets_1.append(data['metrics']['citations']['total'])\n",
    "#             num_followers_1.append(data['author']['followers'])\n",
    "#         elif (time <= time2):\n",
    "#             posting_time_2.append(data['citation_date'])\n",
    "#             num_retweets_2.append(data['metrics']['citations']['total'])\n",
    "#             num_followers_2.append(data['author']['followers'])\n",
    "#         else:\n",
    "#             posting_time_3.append(data['citation_date'])\n",
    "#             num_retweets_3.append(data['metrics']['citations']['total'])\n",
    "#             num_followers_3.append(data['author']['followers'])\n",
    "        \n",
    "#     file.close()\n",
    "    \n",
    "#     hours1 = int((max(posting_time_1)-min(posting_time_1))/3600)+1\n",
    "#     tmp1 = np.zeros([hours1, 5])\n",
    "#     start_time_1 = min(posting_time_1)\n",
    "#     start_hour_1 = (datetime.datetime.fromtimestamp(start_time_1, pst_tz)).hour\n",
    "    \n",
    "#     fivemins = int((max(posting_time_2)-min(posting_time_2))/300)+1\n",
    "#     tmp2 = np.zeros([fivemins, 5])\n",
    "#     start_time_2 = min(posting_time_2)\n",
    "#     start_5min_2 = int(((datetime.datetime.fromtimestamp(start_time_2, pst_tz)).minute)/5)\n",
    "    \n",
    "#     hours3 = int((max(posting_time_3)-min(posting_time_3))/3600)+1\n",
    "#     tmp3 = np.zeros([hours3, 5])\n",
    "#     start_time_3 = min(posting_time_3)\n",
    "#     start_hour_3 = (datetime.datetime.fromtimestamp(start_time_3, pst_tz)).hour\n",
    "\n",
    "#     for i in range(hours1):\n",
    "#         tmp1[i,4] = (start_hour_1+i)%24\n",
    "#     for i in range(len(posting_time_1)):\n",
    "#         tmp1[int((posting_time_1[i]-start_time_1)/3600), 0] += 1\n",
    "#         tmp1[int((posting_time_1[i]-start_time_1)/3600), 1] += num_retweets_1[i]\n",
    "#         tmp1[int((posting_time_1[i]-start_time_1)/3600), 2] += num_followers_1[i]\n",
    "#         if tmp1[int((posting_time_1[i]-start_time_1)/3600), 3] < num_followers_1[i]:\n",
    "#             tmp1[int((posting_time_1[i]-start_time_1)/3600), 3] = num_followers_1[i]\n",
    "            \n",
    "#     for i in range(hours3):\n",
    "#         tmp3[i,4] = (start_hour_3+i)%24\n",
    "#     for i in range(len(posting_time_3)):\n",
    "#         tmp3[int((posting_time_3[i]-start_time_3)/3600), 0] += 1\n",
    "#         tmp3[int((posting_time_3[i]-start_time_3)/3600), 1] += num_retweets_3[i]\n",
    "#         tmp3[int((posting_time_3[i]-start_time_3)/3600), 2] += num_followers_3[i]\n",
    "#         if tmp3[int((posting_time_3[i]-start_time_3)/3600), 3] < num_followers_3[i]:\n",
    "#             tmp3[int((posting_time_3[i]-start_time_3)/3600), 3] = num_followers_3[i]\n",
    "    \n",
    "#     for i in range(fivemins):\n",
    "#         tmp2[i,4] = (start_5min_2+i)%12\n",
    "#     for i in range(len(posting_time_2)):\n",
    "#         tmp2[int((posting_time_2[i]-start_time_2)/300), 0] += 1\n",
    "#         tmp2[int((posting_time_2[i]-start_time_2)/300), 1] += num_retweets_2[i]\n",
    "#         tmp2[int((posting_time_2[i]-start_time_2)/300), 2] += num_followers_2[i]\n",
    "#         if tmp2[int((posting_time_2[i]-start_time_2)/300), 3] < num_followers_2[i]:\n",
    "#             tmp2[int((posting_time_2[i]-start_time_2)/300), 3] = num_followers_2[i]\n",
    "            \n",
    "            \n",
    "#     return (tmp1, tmp2, tmp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting features for aggregate\n",
    "\n",
    "def extracting_5_3_agg():\n",
    "    pst_tz = pytz.timezone('America/Los_Angeles')\n",
    "    \n",
    "    posting_time_1 = []\n",
    "    num_retweets_1 = []\n",
    "    num_followers_1 = []\n",
    "    \n",
    "    posting_time_2 = []\n",
    "    num_retweets_2 = []\n",
    "    num_followers_2 = []\n",
    "    \n",
    "    posting_time_3 = []\n",
    "    num_retweets_3 = []\n",
    "    num_followers_3 = []\n",
    "    \n",
    "    \n",
    "    pst_tz = pytz.timezone('America/Los_Angeles')\n",
    "    time1 = datetime.datetime(year = 2015, month = 2, day = 1, hour = 8, tzinfo = pst_tz).timestamp()\n",
    "    time2 = datetime.datetime(year = 2015, month = 2, day = 1, hour = 20, tzinfo = pst_tz).timestamp()\n",
    "    \n",
    "    hashtags = ['tweets_#gohawks.txt', 'tweets_#gopatriots.txt', 'tweets_#nfl.txt', 'tweets_#patriots.txt', 'tweets_#sb49.txt', 'tweets_#superbowl.txt']\n",
    "    for hashtag in hashtags:\n",
    "        print (\"Opening: \", hashtag)\n",
    "        file = open('./data/'+hashtag, encoding = 'utf8')\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            time = data['citation_date']\n",
    "            if (time < time1):\n",
    "                posting_time_1.append(data['citation_date'])\n",
    "                num_retweets_1.append(data['metrics']['citations']['total'])\n",
    "                num_followers_1.append(data['author']['followers'])\n",
    "            elif (time <= time2):\n",
    "                posting_time_2.append(data['citation_date'])\n",
    "                num_retweets_2.append(data['metrics']['citations']['total'])\n",
    "                num_followers_2.append(data['author']['followers'])\n",
    "            else:\n",
    "                posting_time_3.append(data['citation_date'])\n",
    "                num_retweets_3.append(data['metrics']['citations']['total'])\n",
    "                num_followers_3.append(data['author']['followers'])\n",
    "\n",
    "        file.close()\n",
    "    \n",
    "    hours1 = int((max(posting_time_1)-min(posting_time_1))/3600)+1\n",
    "    tmp1 = np.zeros([hours1, 5])\n",
    "    start_time_1 = min(posting_time_1)\n",
    "    start_hour_1 = (datetime.datetime.fromtimestamp(start_time_1, pst_tz)).hour\n",
    "    \n",
    "    fivemins = int((max(posting_time_2)-min(posting_time_2))/300)+1\n",
    "    tmp2 = np.zeros([fivemins, 5])\n",
    "    start_time_2 = min(posting_time_2)\n",
    "    start_5min_2 = int(((datetime.datetime.fromtimestamp(start_time_2, pst_tz)).minute)/5)\n",
    "    \n",
    "    hours3 = int((max(posting_time_3)-min(posting_time_3))/3600)+1\n",
    "    tmp3 = np.zeros([hours3, 5])\n",
    "    start_time_3 = min(posting_time_3)\n",
    "    start_hour_3 = (datetime.datetime.fromtimestamp(start_time_3, pst_tz)).hour\n",
    "\n",
    "    for i in range(hours1):\n",
    "        tmp1[i,4] = (start_hour_1+i)%24\n",
    "    for i in range(len(posting_time_1)):\n",
    "        tmp1[int((posting_time_1[i]-start_time_1)/3600), 0] += 1\n",
    "        tmp1[int((posting_time_1[i]-start_time_1)/3600), 1] += num_retweets_1[i]\n",
    "        tmp1[int((posting_time_1[i]-start_time_1)/3600), 2] += num_followers_1[i]\n",
    "        if tmp1[int((posting_time_1[i]-start_time_1)/3600), 3] < num_followers_1[i]:\n",
    "            tmp1[int((posting_time_1[i]-start_time_1)/3600), 3] = num_followers_1[i]\n",
    "            \n",
    "    for i in range(hours3):\n",
    "        tmp3[i,4] = (start_hour_3+i)%24\n",
    "    for i in range(len(posting_time_3)):\n",
    "        tmp3[int((posting_time_3[i]-start_time_3)/3600), 0] += 1\n",
    "        tmp3[int((posting_time_3[i]-start_time_3)/3600), 1] += num_retweets_3[i]\n",
    "        tmp3[int((posting_time_3[i]-start_time_3)/3600), 2] += num_followers_3[i]\n",
    "        if tmp3[int((posting_time_3[i]-start_time_3)/3600), 3] < num_followers_3[i]:\n",
    "            tmp3[int((posting_time_3[i]-start_time_3)/3600), 3] = num_followers_3[i]\n",
    "    \n",
    "    for i in range(fivemins):\n",
    "        tmp2[i,4] = (start_5min_2+i)%12\n",
    "    for i in range(len(posting_time_2)):\n",
    "        tmp2[int((posting_time_2[i]-start_time_2)/300), 0] += 1\n",
    "        tmp2[int((posting_time_2[i]-start_time_2)/300), 1] += num_retweets_2[i]\n",
    "        tmp2[int((posting_time_2[i]-start_time_2)/300), 2] += num_followers_2[i]\n",
    "        if tmp2[int((posting_time_2[i]-start_time_2)/300), 3] < num_followers_2[i]:\n",
    "            tmp2[int((posting_time_2[i]-start_time_2)/300), 3] = num_followers_2[i]\n",
    "            \n",
    "            \n",
    "    return (tmp1, tmp2, tmp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_features_agg(): \n",
    "#     posting_time = []\n",
    "    \n",
    "    \n",
    "#     hashtags = ['tweets_#gohawks.txt', 'tweets_#gopatriots.txt', 'tweets_#nfl.txt', 'tweets_#patriots.txt', 'tweets_#sb49.txt', 'tweets_#superbowl.txt']\n",
    "#     for hashtag in hashtags:\n",
    "#         print (\"Opening: \", hashtag)\n",
    "#         file = open('./data/'+hashtag, encoding = 'utf8')\n",
    "#         for line in file:\n",
    "#             data = json.loads(line)\n",
    "#             posting_time.append(data['citation_date'])\n",
    "#         file.close()\n",
    "    \n",
    "#    total_hours = int((max(posting_time)-min(posting_time))/3600)+1\n",
    "#     #total_hours = (end_time - start_time) / 3600 + 1\n",
    "#     start_time = min(posting_time)\n",
    "#     end_time= max(posting_time)\n",
    "    \n",
    "#     # extract five basic features\n",
    "#     feat_5 = extracting_agg()\n",
    "    \n",
    "#     ori_author_followers = [0 for i in range(total_hours)] # total number of followers of the original author\n",
    "#     favorited_num = [0 for i in range(total_hours)] # total number of times of favorited, so many zeros\n",
    "#     impressions_num = [0 for i in range(total_hours)]\n",
    "#     avg_ranking_score = [0 for i in range(total_hours)]\n",
    "#     user_mentions = [0 for i in range(total_hours)]\n",
    "#     url_count = [0 for i in range(total_hours)]\n",
    "#     unique_author_set = [set() for i in range(total_hours)] # save unique author \n",
    "    \n",
    "#     hashtags = ['tweets_#gohawks.txt', 'tweets_#gopatriots.txt', 'tweets_#nfl.txt', 'tweets_#patriots.txt', 'tweets_#sb49.txt', 'tweets_#superbowl.txt']\n",
    "#     for hashtag in hashtags:\n",
    "#         print ('Reading from:' , hashtag)\n",
    "#         file = open('./data/'+hashtag, encoding = 'utf8')\n",
    "#         # extract other features\n",
    "#         for line in file:\n",
    "#             tweet = json.loads(line)\n",
    "#             tweet_time = tweet['citation_date']\n",
    "#             hour = int((tweet_time - start_time) / 3600)\n",
    "#             ori_author_followers[hour] += tweet['original_author']['followers']\n",
    "#             favorited_num[hour] += tweet['tweet']['favorite_count']\n",
    "\n",
    "#             user_mentions[hour] += len(tweet['tweet']['entities']['user_mentions'])\n",
    "#             url_count[hour] += len(tweet['tweet']['entities']['urls'])\n",
    "#             unique_author_set[hour].add(tweet['author']['nick'])\n",
    "#             impressions_num[hour] += tweet['metrics']['impressions']\n",
    "#             avg_ranking_score[hour] += tweet['metrics']['ranking_score'] # need to do average later\n",
    "#         file.close() \n",
    "    \n",
    "#     total_tweets = feat_5[0]\n",
    "#     for i in range(0, len(total_tweets)):\n",
    "#         if(total_tweets[i] != 0):\n",
    "#             avg_ranking_score[i] = avg_ranking_score[i] / total_tweets[i]\n",
    "    \n",
    "#     unique_author_count =  [len(val) for val in unique_author_set]  # number of unique authors\n",
    "#     feat_extra =np.array([ori_author_followers, favorited_num, \\\n",
    "#                   user_mentions, url_count, unique_author_count,\\\n",
    "#                   impressions_num,avg_ranking_score]).T\n",
    "\n",
    "    \n",
    "#     print(feat_extra.shape)\n",
    "#     feat_all = np.hstack((feat_5 , feat_extra))\n",
    "#     print(feat_all[0,:])\n",
    "#     return feat_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_features_3(hashtag): \n",
    "#     posting_time = []\n",
    "#     file = open('./data/'+hashtag, encoding = 'utf8')\n",
    "    \n",
    "#     pst_tz = pytz.timezone('America/Los_Angeles')\n",
    "#     time1 = datetime.datetime(year = 2015, month = 2, day = 1, hour = 8, tzinfo = pst_tz).timestamp()\n",
    "#     time2 = datetime.datetime(year = 2015, month = 2, day = 1, hour = 20, tzinfo = pst_tz).timestamp()\n",
    "    \n",
    "#     period1_raw = []\n",
    "#     period2_raw = []\n",
    "#     period3_raw = []\n",
    "    \n",
    "#     for line in file:\n",
    "#         data = json.loads(line)\n",
    "#         time = data['citation_date']\n",
    "#         if (time < time1):\n",
    "#             period1_raw.append(time)\n",
    "#         elif (time <= time2):\n",
    "#             period2_raw.append(time)\n",
    "#         else:\n",
    "#             period3_raw.append(time)\n",
    "    \n",
    "#     total_hours_1 = int((max(period1_raw)-min(period1_raw))/3600)+1\n",
    "#     start_time_1 = min(period1_raw)\n",
    "#     end_time_1 = max(period1_raw)\n",
    "    \n",
    "#     total_5min_2 = int((max(period2_raw)-min(period2_raw))/300)+1\n",
    "#     start_time_2 = min(period2_raw)\n",
    "#     end_time_2 = max(period2_raw)\n",
    "    \n",
    "#     total_hours_3 = int((max(period3_raw)-min(period3_raw))/3600)+1\n",
    "#     start_time_3 = min(period3_raw)\n",
    "#     end_time_3 = max(period3_raw)\n",
    "    \n",
    "#     # extract five basic features\n",
    "#     feat_5_1, feat_5_2, feat_5_3 = extracting_3(hashtag)\n",
    "    \n",
    "#     ori_author_followers_1 = [0 for i in range(total_hours_1)] # total number of followers of the original author\n",
    "#     favorited_num_1 = [0 for i in range(total_hours_1)] # total number of times of favorited, so many zeros\n",
    "#     impressions_num_1 = [0 for i in range(total_hours_1)]\n",
    "#     avg_ranking_score_1 = [0 for i in range(total_hours_1)]\n",
    "#     user_mentions_1 = [0 for i in range(total_hours_1)]\n",
    "#     url_count_1 = [0 for i in range(total_hours_1)]\n",
    "#     unique_author_set_1 = [set() for i in range(total_hours_1)] # save unique author \n",
    "    \n",
    "#     ori_author_followers_2 = [0 for i in range(total_5min_2)] # total number of followers of the original author\n",
    "#     favorited_num_2 = [0 for i in range(total_5min_2)] # total number of times of favorited, so many zeros\n",
    "#     impressions_num_2 = [0 for i in range(total_5min_2)]\n",
    "#     avg_ranking_score_2 = [0 for i in range(total_5min_2)]\n",
    "#     user_mentions_2 = [0 for i in range(total_5min_2)]\n",
    "#     url_count_2 = [0 for i in range(total_5min_2)]\n",
    "#     unique_author_set_2 = [set() for i in range(total_5min_2)] # save unique author \n",
    "    \n",
    "#     ori_author_followers_3 = [0 for i in range(total_hours_3)] # total number of followers of the original author\n",
    "#     favorited_num_3 = [0 for i in range(total_hours_3)] # total number of times of favorited, so many zeros\n",
    "#     impressions_num_3 = [0 for i in range(total_hours_3)]\n",
    "#     avg_ranking_score_3 = [0 for i in range(total_hours_3)]\n",
    "#     user_mentions_3 = [0 for i in range(total_hours_3)]\n",
    "#     url_count_3 = [0 for i in range(total_hours_3)]\n",
    "#     unique_author_set_3 = [set() for i in range(total_hours_3)] # save unique author \n",
    "    \n",
    "#     file = open('./data/'+hashtag, encoding = 'utf8')\n",
    "#     # extract other features\n",
    "#     for line in file:\n",
    "#         tweet = json.loads(line)\n",
    "#         tweet_time = tweet['citation_date']\n",
    "        \n",
    "#         unit = 0\n",
    "#         if (tweet_time < time1):\n",
    "#             unit = int((tweet_time - start_time_1) / 3600)\n",
    "#             ori_author_followers_1[unit] += tweet['original_author']['followers']\n",
    "#             favorited_num_1[unit] += tweet['tweet']['favorite_count']\n",
    "#             user_mentions_1[unit] += len(tweet['tweet']['entities']['user_mentions'])\n",
    "#             url_count_1[unit] += len(tweet['tweet']['entities']['urls'])\n",
    "#             unique_author_set_1[unit].add(tweet['author']['nick'])\n",
    "#             impressions_num_1[unit] += tweet['metrics']['impressions']\n",
    "#             avg_ranking_score_1[unit] += tweet['metrics']['ranking_score'] # need to do average later\n",
    "#         elif (tweet_time <= time2):\n",
    "#             unit = int((tweet_time - start_time_2) / 300)\n",
    "#             ori_author_followers_2[unit] += tweet['original_author']['followers']\n",
    "#             favorited_num_2[unit] += tweet['tweet']['favorite_count']\n",
    "#             user_mentions_2[unit] += len(tweet['tweet']['entities']['user_mentions'])\n",
    "#             url_count_2[unit] += len(tweet['tweet']['entities']['urls'])\n",
    "#             unique_author_set_2[unit].add(tweet['author']['nick'])\n",
    "#             impressions_num_2[unit] += tweet['metrics']['impressions']\n",
    "#             avg_ranking_score_2[unit] += tweet['metrics']['ranking_score'] # need to do average later\n",
    "#         else:\n",
    "#             unit = int((tweet_time - start_time_3) / 3600)\n",
    "#             ori_author_followers_3[unit] += tweet['original_author']['followers']\n",
    "#             favorited_num_3[unit] += tweet['tweet']['favorite_count']\n",
    "#             user_mentions_3[unit] += len(tweet['tweet']['entities']['user_mentions'])\n",
    "#             url_count_3[unit] += len(tweet['tweet']['entities']['urls'])\n",
    "#             unique_author_set_3[unit].add(tweet['author']['nick'])\n",
    "#             impressions_num_3[unit] += tweet['metrics']['impressions']\n",
    "#             avg_ranking_score_3[unit] += tweet['metrics']['ranking_score'] # need to do average later\n",
    "        \n",
    "           \n",
    "#     file.close()   \n",
    "    \n",
    "#     total_tweets = feat_5_1[0]\n",
    "#     for i in range(0, len(total_tweets)):\n",
    "#         if(total_tweets[i] != 0):\n",
    "#             avg_ranking_score_1[i] = avg_ranking_score_1[i] / total_tweets[i]\n",
    "    \n",
    "#     total_tweets = feat_5_2[0]\n",
    "#     for i in range(0, len(total_tweets)):\n",
    "#         if(total_tweets[i] != 0):\n",
    "#             avg_ranking_score_2[i] = avg_ranking_score_2[i] / total_tweets[i]\n",
    "            \n",
    "#     total_tweets = feat_5_3[0]\n",
    "#     for i in range(0, len(total_tweets)):\n",
    "#         if(total_tweets[i] != 0):\n",
    "#             avg_ranking_score_3[i] = avg_ranking_score_3[i] / total_tweets[i]\n",
    "    \n",
    "#     unique_author_count_1 =  [len(val) for val in unique_author_set_1]  # number of unique authors\n",
    "#     unique_author_count_2 =  [len(val) for val in unique_author_set_2]  # number of unique authors\n",
    "#     unique_author_count_3 =  [len(val) for val in unique_author_set_3]  # number of unique authors\n",
    "    \n",
    "#     feat_extra_1 =np.array([ori_author_followers_1, favorited_num_1, \\\n",
    "#                   user_mentions_1, url_count_1, unique_author_count_1,\\\n",
    "#                   impressions_num_1,avg_ranking_score_1]).T\n",
    "    \n",
    "#     feat_extra_2 =np.array([ori_author_followers_2, favorited_num_2, \\\n",
    "#                   user_mentions_2, url_count_2, unique_author_count_2,\\\n",
    "#                   impressions_num_2,avg_ranking_score_2]).T\n",
    "    \n",
    "#     feat_extra_3 =np.array([ori_author_followers_3, favorited_num_3, \\\n",
    "#                   user_mentions_3, url_count_3, unique_author_count_3,\\\n",
    "#                   impressions_num_3,avg_ranking_score_3]).T\n",
    "\n",
    "    \n",
    "#     print(feat_extra_1.shape)\n",
    "#     print(feat_extra_2.shape)\n",
    "#     print(feat_extra_3.shape)\n",
    "    \n",
    "\n",
    "#     feat_all_1 = np.hstack((feat_5_1 , feat_extra_1))\n",
    "#     feat_all_2 = np.hstack((feat_5_2 , feat_extra_2))\n",
    "#     feat_all_3 = np.hstack((feat_5_3 , feat_extra_3))\n",
    "    \n",
    "    \n",
    "#     return feat_all_1, feat_all_2, feat_all_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For aggregate\n",
    "# def extract_features_3_agg(): \n",
    "#     posting_time = []\n",
    "    \n",
    "#     pst_tz = pytz.timezone('America/Los_Angeles')\n",
    "#     time1 = datetime.datetime(year = 2015, month = 2, day = 1, hour = 8, tzinfo = pst_tz).timestamp()\n",
    "#     time2 = datetime.datetime(year = 2015, month = 2, day = 1, hour = 20, tzinfo = pst_tz).timestamp()\n",
    "    \n",
    "#     period1_raw = []\n",
    "#     period2_raw = []\n",
    "#     period3_raw = []\n",
    "    \n",
    "#     hashtags = ['tweets_#gohawks.txt', 'tweets_#gopatriots.txt', 'tweets_#nfl.txt', 'tweets_#patriots.txt', 'tweets_#sb49.txt', 'tweets_#superbowl.txt']\n",
    "#     for hashtag in hashtags:\n",
    "#         file = open('./data/'+hashtag, encoding = 'utf8')\n",
    "#         print (\"Reading: \", hashtag)\n",
    "#         for line in file:\n",
    "#             data = json.loads(line)\n",
    "#             time = data['citation_date']\n",
    "#             if (time < time1):\n",
    "#                 period1_raw.append(time)\n",
    "#             elif (time <= time2):\n",
    "#                 period2_raw.append(time)\n",
    "#             else:\n",
    "#                 period3_raw.append(time)\n",
    "#         file.close()\n",
    "    \n",
    "#     total_hours_1 = int((max(period1_raw)-min(period1_raw))/3600)+1\n",
    "#     start_time_1 = min(period1_raw)\n",
    "#     end_time_1 = max(period1_raw)\n",
    "    \n",
    "#     total_5min_2 = int((max(period2_raw)-min(period2_raw))/300)+1\n",
    "#     start_time_2 = min(period2_raw)\n",
    "#     end_time_2 = max(period2_raw)\n",
    "    \n",
    "#     total_hours_3 = int((max(period3_raw)-min(period3_raw))/3600)+1\n",
    "#     print (\"Total hours 3 outside is:\", total_hours_3)\n",
    "#     start_time_3 = min(period3_raw)\n",
    "#     end_time_3 = max(period3_raw)\n",
    "    \n",
    "#     # extract five basic features\n",
    "#     feat_5_1, feat_5_2, feat_5_3 = extracting_3_agg()\n",
    "    \n",
    "#     ori_author_followers_1 = [0 for i in range(total_hours_1)] # total number of followers of the original author\n",
    "#     favorited_num_1 = [0 for i in range(total_hours_1)] # total number of times of favorited, so many zeros\n",
    "#     impressions_num_1 = [0 for i in range(total_hours_1)]\n",
    "#     avg_ranking_score_1 = [0 for i in range(total_hours_1)]\n",
    "#     user_mentions_1 = [0 for i in range(total_hours_1)]\n",
    "#     url_count_1 = [0 for i in range(total_hours_1)]\n",
    "#     unique_author_set_1 = [set() for i in range(total_hours_1)] # save unique author \n",
    "    \n",
    "#     ori_author_followers_2 = [0 for i in range(total_5min_2)] # total number of followers of the original author\n",
    "#     favorited_num_2 = [0 for i in range(total_5min_2)] # total number of times of favorited, so many zeros\n",
    "#     impressions_num_2 = [0 for i in range(total_5min_2)]\n",
    "#     avg_ranking_score_2 = [0 for i in range(total_5min_2)]\n",
    "#     user_mentions_2 = [0 for i in range(total_5min_2)]\n",
    "#     url_count_2 = [0 for i in range(total_5min_2)]\n",
    "#     unique_author_set_2 = [set() for i in range(total_5min_2)] # save unique author \n",
    "    \n",
    "#     ori_author_followers_3 = [0 for i in range(total_hours_3)] # total number of followers of the original author\n",
    "#     favorited_num_3 = [0 for i in range(total_hours_3)] # total number of times of favorited, so many zeros\n",
    "#     impressions_num_3 = [0 for i in range(total_hours_3)]\n",
    "#     avg_ranking_score_3 = [0 for i in range(total_hours_3)]\n",
    "#     user_mentions_3 = [0 for i in range(total_hours_3)]\n",
    "#     url_count_3 = [0 for i in range(total_hours_3)]\n",
    "#     unique_author_set_3 = [set() for i in range(total_hours_3)] # save unique author \n",
    "    \n",
    "    \n",
    "#     hashtags = ['tweets_#gohawks.txt', 'tweets_#gopatriots.txt', 'tweets_#nfl.txt', 'tweets_#patriots.txt', 'tweets_#sb49.txt', 'tweets_#superbowl.txt']\n",
    "#     for hashtag in hashtags:\n",
    "#         print (\"Opening: \", hashtag)\n",
    "#         file = open('./data/'+hashtag, encoding = 'utf8')\n",
    "#         # extract other features\n",
    "#         for line in file:\n",
    "#             tweet = json.loads(line)\n",
    "#             tweet_time = tweet['citation_date']\n",
    "\n",
    "#             unit = 0\n",
    "#             if (tweet_time < time1):\n",
    "#                 unit = int((tweet_time - start_time_1) / 3600)\n",
    "#                 ori_author_followers_1[unit] += tweet['original_author']['followers']\n",
    "#                 favorited_num_1[unit] += tweet['tweet']['favorite_count']\n",
    "#                 user_mentions_1[unit] += len(tweet['tweet']['entities']['user_mentions'])\n",
    "#                 url_count_1[unit] += len(tweet['tweet']['entities']['urls'])\n",
    "#                 unique_author_set_1[unit].add(tweet['author']['nick'])\n",
    "#                 impressions_num_1[unit] += tweet['metrics']['impressions']\n",
    "#                 avg_ranking_score_1[unit] += tweet['metrics']['ranking_score'] # need to do average later\n",
    "#             elif (tweet_time <= time2):\n",
    "#                 unit = int((tweet_time - start_time_2) / 300)\n",
    "#                 ori_author_followers_2[unit] += tweet['original_author']['followers']\n",
    "#                 favorited_num_2[unit] += tweet['tweet']['favorite_count']\n",
    "#                 user_mentions_2[unit] += len(tweet['tweet']['entities']['user_mentions'])\n",
    "#                 url_count_2[unit] += len(tweet['tweet']['entities']['urls'])\n",
    "#                 unique_author_set_2[unit].add(tweet['author']['nick'])\n",
    "#                 impressions_num_2[unit] += tweet['metrics']['impressions']\n",
    "#                 avg_ranking_score_2[unit] += tweet['metrics']['ranking_score'] # need to do average later\n",
    "#             else:\n",
    "#                 unit = int((tweet_time - start_time_3) / 3600)\n",
    "#                 ori_author_followers_3[unit] += tweet['original_author']['followers']\n",
    "#                 favorited_num_3[unit] += tweet['tweet']['favorite_count']\n",
    "#                 user_mentions_3[unit] += len(tweet['tweet']['entities']['user_mentions'])\n",
    "#                 url_count_3[unit] += len(tweet['tweet']['entities']['urls'])\n",
    "#                 unique_author_set_3[unit].add(tweet['author']['nick'])\n",
    "#                 impressions_num_3[unit] += tweet['metrics']['impressions']\n",
    "#                 avg_ranking_score_3[unit] += tweet['metrics']['ranking_score'] # need to do average later\n",
    "#         file.close()   \n",
    "    \n",
    "    \n",
    "#     total_tweets = feat_5_1[0]\n",
    "#     for i in range(0, len(total_tweets)):\n",
    "#         if(total_tweets[i] != 0):\n",
    "#             avg_ranking_score_1[i] = avg_ranking_score_1[i] / total_tweets[i]\n",
    "    \n",
    "#     total_tweets = feat_5_2[0]\n",
    "#     for i in range(0, len(total_tweets)):\n",
    "#         if(total_tweets[i] != 0):\n",
    "#             avg_ranking_score_2[i] = avg_ranking_score_2[i] / total_tweets[i]\n",
    "            \n",
    "#     total_tweets = feat_5_3[0]\n",
    "#     for i in range(0, len(total_tweets)):\n",
    "#         if(total_tweets[i] != 0):\n",
    "#             avg_ranking_score_3[i] = avg_ranking_score_3[i] / total_tweets[i]\n",
    "    \n",
    "#     unique_author_count_1 =  [len(val) for val in unique_author_set_1]  # number of unique authors\n",
    "#     unique_author_count_2 =  [len(val) for val in unique_author_set_2]  # number of unique authors\n",
    "#     unique_author_count_3 =  [len(val) for val in unique_author_set_3]  # number of unique authors\n",
    "    \n",
    "#     feat_extra_1 =np.array([ori_author_followers_1, favorited_num_1, \\\n",
    "#                   user_mentions_1, url_count_1, unique_author_count_1,\\\n",
    "#                   impressions_num_1,avg_ranking_score_1]).T\n",
    "    \n",
    "#     feat_extra_2 =np.array([ori_author_followers_2, favorited_num_2, \\\n",
    "#                   user_mentions_2, url_count_2, unique_author_count_2,\\\n",
    "#                   impressions_num_2,avg_ranking_score_2]).T\n",
    "    \n",
    "#     feat_extra_3 =np.array([ori_author_followers_3, favorited_num_3, \\\n",
    "#                   user_mentions_3, url_count_3, unique_author_count_3,\\\n",
    "#                   impressions_num_3,avg_ranking_score_3]).T\n",
    "\n",
    "    \n",
    "#     print(feat_extra_1.shape)\n",
    "#     print(feat_extra_2.shape)\n",
    "#     print(feat_extra_3.shape)\n",
    "    \n",
    "#     print(feat_5_1.shape)\n",
    "#     print(feat_5_2.shape)\n",
    "#     print(feat_5_3.shape)\n",
    "    \n",
    "#     feat_all_1 = np.hstack((feat_5_1 , feat_extra_1))\n",
    "#     feat_all_2 = np.hstack((feat_5_2 , feat_extra_2))\n",
    "#     feat_all_3 = np.hstack((feat_5_3 , feat_extra_3))\n",
    "    \n",
    "    \n",
    "#     return feat_all_1, feat_all_2, feat_all_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from file  tweets_#gohawks.txt\n",
      "Reading from file  tweets_#gopatriots.txt\n",
      "Reading from file  tweets_#nfl.txt\n",
      "Reading from file  tweets_#patriots.txt\n",
      "Reading from file  tweets_#sb49.txt\n",
      "Reading from file  tweets_#superbowl.txt\n"
     ]
    }
   ],
   "source": [
    "data_agg = extracting_5_agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"data_agg.pkl\",\"wb\")\n",
    "pickle.dump(data_agg, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"data_agg.pkl\",\"rb\")\n",
    "data_agg = pickle.load(pickle_in)\n",
    "pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening:  tweets_#gohawks.txt\n",
      "Opening:  tweets_#gopatriots.txt\n",
      "Opening:  tweets_#nfl.txt\n",
      "Opening:  tweets_#patriots.txt\n",
      "Opening:  tweets_#sb49.txt\n",
      "Opening:  tweets_#superbowl.txt\n"
     ]
    }
   ],
   "source": [
    "data1_agg, data2_agg, data3_agg = extracting_5_3_agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"data1_agg.pkl\",\"wb\")\n",
    "pickle.dump(data1_agg, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data2_agg.pkl\",\"wb\")\n",
    "pickle.dump(data2_agg, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data3_agg.pkl\",\"wb\")\n",
    "pickle.dump(data3_agg, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitting(data_all, layer_sizes = (100,) , scaling = False, k = 5):\n",
    "    kf = KFold(n_splits=5)\n",
    "    \n",
    "    data = data_all[:-1, :]\n",
    "    target = data_all[1:, 0]\n",
    "    \n",
    "    if (scaling):\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(data)\n",
    "        data = scaler.transform(data)\n",
    "    \n",
    "    mse = 0.0\n",
    "\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = target[train_index], target[test_index]\n",
    "        \n",
    "        model = MLPRegressor(hidden_layer_sizes = layer_sizes)\n",
    "        results = model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        mse += mean_squared_error(pred, y_test)\n",
    "    \n",
    "    mse = mse/float(k)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer:  (100, 200, 100)\n",
      "Average MSE: 71802158931.05899\n",
      "Layer:  (200, 200, 200)\n",
      "Average MSE: 11345012551.1648\n",
      "Layer:  (400, 400, 400)\n",
      "Average MSE: 4707155944.130046\n",
      "Layer:  (100, 100, 100, 100)\n",
      "Average MSE: 744908763.3402637\n",
      "Layer:  (100, 200, 200, 100)\n",
      "Average MSE: 2561310763.3898077\n",
      "Layer:  (100, 100, 100, 100, 100)\n",
      "Average MSE: 1829384574.5866916\n",
      "Layer:  (100, 400, 400, 400, 100)\n",
      "Average MSE: 1638897368.7401216\n"
     ]
    }
   ],
   "source": [
    "# Question 11\n",
    "layers = [(100,200,100), (200,200,200), (400,400,400),  (100, 100, 100, 100), (100,200,200,100), (100,100,100,100,100), (100,400,400,400,100)]\n",
    "mses = []\n",
    "for layer in layers:\n",
    "    print (\"Layer: \" , layer)\n",
    "    mse = fitting(data_agg, layer_sizes=layer)\n",
    "    mses.append(mse)\n",
    "    print (\"Average MSE:\" , mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "744908763.3402637\n"
     ]
    }
   ],
   "source": [
    "print (min(mses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125130947.2827256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Question 12 \n",
    "best_layer = (100, 100, 100, 100, 100)\n",
    "mse = fitting(data_agg, layer_sizes=layer, scaling = True)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q13_grid_search(data_all):\n",
    "    data = data_all[:-1, :]\n",
    "    target = data_all[1:, 0]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data)\n",
    "    data = scaler.transform(data)\n",
    "    \n",
    "    parameters = {'hidden_layer_sizes': [(100,200,100), (200,200,200), (400,400,400),  (100, 100, 100, 100), (100,200,200,100), (100,100,100,100,100), (100,400,400,400,100)]}\n",
    "    mlp = MLPRegressor()\n",
    "    clf = GridSearchCV(mlp, parameters, cv = KFold(5, shuffle=True), scoring='neg_mean_squared_error')\n",
    "    clf.fit(data, target)\n",
    "    \n",
    "    y_pred =  clf.predict(data)\n",
    "    mse = mean_squared_error(y_pred, target)\n",
    "    print (\"Mean Squared Error for Best Model is: \" , mse)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for Best Model is:  3511566.426449395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Period 1\n",
    "q13_grid_search(data1_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for Best Model is:  22737832.84784201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Period 2\n",
    "q13_grid_search(data2_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for Best Model is:  148549.38749894637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asavari\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Period 3\n",
    "q13_grid_search(data3_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant features from 6 time period window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the best model for each of the three time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Best Model\n",
    "\n",
    "data = data_agg[:-1, :]\n",
    "target = data_agg[1:, 0]\n",
    "\n",
    "layer_sizes = (100,100,100,100,100)\n",
    "model = MLPRegressor(hidden_layer_sizes = layer_sizes)\n",
    "results = model.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse for file  sample0_period1.txt is  678366.2543027672\n",
      "Predictions:  [  -4.95262085 1812.84865245 -141.44065159 -329.00489745 -293.98247025]\n",
      "Values:  [ 77.  94. 102. 121. 117.]\n",
      "\n",
      "\n",
      "mse for file  sample1_period1.txt is  2969258.070832409\n",
      "Predictions:  [-111.62026472 1599.87929153 3262.30271784 -318.82379132 -951.99677937]\n",
      "Values:  [180. 202. 294. 555. 846.]\n",
      "\n",
      "\n",
      "mse for file  sample2_period1.txt is  882528.5065944813\n",
      "Predictions:  [1730.94342866  852.54452271  -32.71466923   30.92509108 1193.75150005]\n",
      "Values:  [141. 101. 145. 103.  61.]\n",
      "\n",
      "\n",
      "\n",
      "##################################################################################\n",
      "\n",
      "mse for file  sample0_period2.txt is  19740554.569248628\n",
      "Predictions:  [-2021.87666646 -4596.7709249  -1777.40438414  -746.21018704\n",
      "  -623.41489133]\n",
      "Values:  [3834. 2258. 1455. 1235. 1123.]\n",
      "\n",
      "\n",
      "mse for file  sample1_period2.txt is  8541691.582446355\n",
      "Predictions:  [-243.33188997 1183.79837295 1768.54010267 7127.80009679 -169.38149598]\n",
      "Values:  [995. 870. 960. 861. 903.]\n",
      "\n",
      "\n",
      "mse for file  sample2_period2.txt is  32148.72074221129\n",
      "Predictions:  [ 143.84321171   -8.0849821   132.79845003 -227.84848638  287.44307238]\n",
      "Values:  [19. 25. 29. 28. 27.]\n",
      "\n",
      "\n",
      "\n",
      "##################################################################################\n",
      "\n",
      "mse for file  sample0_period3.txt is  617324.3120602947\n",
      "Predictions:  [-1669.11563428    14.74864954   -15.66055839   -22.47855337\n",
      "   430.97303779]\n",
      "Values:  [48. 95. 45. 76. 87.]\n",
      "\n",
      "\n",
      "mse for file  sample1_period3.txt is  6204924.373620947\n",
      "Predictions:  [-5.45640329e+03  6.53523240e+02  1.62997424e+01 -1.98457960e+01\n",
      " -5.42479973e+00]\n",
      "Values:  [79. 39. 34. 42. 41.]\n",
      "\n",
      "\n",
      "mse for file  sample2_period3.txt is  6451107.791502228\n",
      "Predictions:  [   74.48969547   893.63832024   121.13045224 -5482.9327079\n",
      "   749.01995837]\n",
      "Values:  [90. 40. 58. 87. 43.]\n",
      "\n",
      "\n",
      "\n",
      "##################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predicting on the test data\n",
    "pst_tz = pytz.timezone('America/Los_Angeles')\n",
    "models = [model, model, model]\n",
    "time_periods = [3600, 300, 3600]\n",
    "p_tf = [\n",
    "        [\"sample0_period1.txt\", \"sample1_period1.txt\", \"sample2_period1.txt\"],\n",
    "        [\"sample0_period2.txt\", \"sample1_period2.txt\", \"sample2_period2.txt\"],\n",
    "        [\"sample0_period3.txt\", \"sample1_period3.txt\", \"sample2_period3.txt\"]\n",
    "        ]\n",
    "for c in range(len(p_tf)):\n",
    "    for f in p_tf[c]:\n",
    "        file = open('./test_data/'+ f, encoding = 'utf8')\n",
    "        posting_time = []\n",
    "        num_retweets = []\n",
    "        num_followers = []\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            posting_time.append(data['citation_date'])\n",
    "            num_retweets.append(data['metrics']['citations']['total'])\n",
    "            num_followers.append(data['author']['followers'])\n",
    "        file.close()\n",
    "\n",
    "        units = int((max(posting_time)-min(posting_time))/time_periods[c])+1\n",
    "        tmp = np.zeros([units, 5])\n",
    "        start_time = min(posting_time)\n",
    "        if (time_periods[c] == 3600):\n",
    "            start_unit = (datetime.datetime.fromtimestamp(start_time, pst_tz)).hour\n",
    "            div = 24\n",
    "        else:\n",
    "            start_unit = ((datetime.datetime.fromtimestamp(start_time, pst_tz)).minute)/5\n",
    "            div = 12\n",
    "            \n",
    "        for i in range(units):\n",
    "            tmp[i,4] = (start_unit+i)%div\n",
    "        for i in range(len(posting_time)):\n",
    "            tmp[int((posting_time[i]-start_time)/time_periods[c]), 0] += 1\n",
    "            tmp[int((posting_time[i]-start_time)/time_periods[c]), 1] += num_retweets[i]\n",
    "            tmp[int((posting_time[i]-start_time)/time_periods[c]), 2] += num_followers[i]\n",
    "            if tmp[int((posting_time[i]-start_time)/time_periods[c]), 3] < num_followers[i]:\n",
    "                tmp[int((posting_time[i]-start_time)/time_periods[c]), 3] = num_followers[i]\n",
    "\n",
    "        test_data = tmp[:-1, :]\n",
    "        test_target = tmp[1:, 0]\n",
    "        pred = models[c].predict(test_data)\n",
    "        mse = mean_squared_error(pred, test_target)\n",
    "\n",
    "        print(\"mse for file \", f , \"is \", mse)\n",
    "        print(\"Predictions: \", pred)\n",
    "        print(\"Values: \", test_target)\n",
    "        print(\"\\n\")\n",
    "    print(\"\\n##################################################################################\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use textual context to predict user location\n",
    "# Location: json_object['tweet']['user']['location']\n",
    "# Location: Washington, Massachusets or Neither\n",
    "# Consider only if in washington or massachusets\n",
    "# From #superbowl\n",
    "# Use tweet text\n",
    "# Binary Classifier - Predict location from the text\n",
    "# 3 Different Algorithms\n",
    "# 1. plot ROC curve,\n",
    "# 2. report confusion matrix\n",
    "# 3. calculate accuracy, recall and precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkbounded(s, l1, l2):\n",
    "    right = False\n",
    "    left = False\n",
    "    if (l1 < 0):\n",
    "        left = True\n",
    "    else:\n",
    "        left = not (s[l1].isalpha())\n",
    "      \n",
    "    if (l2 >= len(s)):\n",
    "        right = True\n",
    "    else:\n",
    "        right = not s[l2].isalpha()\n",
    "        \n",
    "    return (left and right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchlist(s, L):\n",
    "    for sub in L:\n",
    "        if sub in s:\n",
    "            loc = s.find(sub)\n",
    "            if (checkbounded(s, loc-1, loc+len(sub))):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wa(s):\n",
    "    wa_list = [\"WA\", \"wa\", \"Wa\", \"Washington state\", \"washington state\", \"Washington State\", \"washington State\", \" Wash.\"]\n",
    "    se_list = [\"seattle\", \"Seattle\"]\n",
    "    state_list = [\"state\", \"State\"]\n",
    "    washington_list = [\"washington\", \"Washington\"]\n",
    "   \n",
    "    s_match = matchlist(s, se_list)\n",
    "    w_match = matchlist(s, washington_list)\n",
    "    wa_match = matchlist(s, wa_list)\n",
    "    state_match = matchlist(s, state_list)\n",
    "    \n",
    "    if ((s_match and w_match) or (w_match and state_match) or wa_match):\n",
    "        return True\n",
    "    if (s == \"Washington\" or s == \"washington\"):\n",
    "        return True\n",
    "    if (\", Washington\" in s or \", washington\" in s or \"Washington, USA\" in s or \"washington, USA\" in s):\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "    \n",
    "def ma(s):\n",
    "    ma_list = [\"Massachusetts\", \"massachusetts\" ,\"MA\", \"ma\" , \"Ma\", \"Mass.\" \"mass.\"]\n",
    "    ma_match = matchlist(s, ma_list)\n",
    "    if (ma_match):\n",
    "        return True\n",
    "    if (s == \"Massachusetts\" or s == \"massachusetts\" or s == \"Mass.\" or s == \"mass.\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from file  tweets_#sb49.txt\n"
     ]
    }
   ],
   "source": [
    "# Extracting data\n",
    "\n",
    "hashtags = ['tweets_#gohawks.txt', 'tweets_#gopatriots.txt', 'tweets_#nfl.txt', 'tweets_#patriots.txt', 'tweets_#sb49.txt', 'tweets_#superbowl.txt']\n",
    "hashtags = ['tweets_#sb49.txt']\n",
    "for hashtag in hashtags:\n",
    "    wa_ma = []\n",
    "    labels = []\n",
    "    print (\"Reading from file \", hashtag)\n",
    "    file = open('./data/'+hashtag, encoding = 'utf8')\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        \n",
    "        loc = data['tweet']['user']['location']\n",
    "        text = data['tweet']['text']\n",
    "        if (wa(loc)):\n",
    "            wa_ma.append(text)\n",
    "            labels.append(0)\n",
    "        elif(ma(loc)):\n",
    "            wa_ma.append(text)\n",
    "            labels.append(1)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8791\n",
      "14545\n"
     ]
    }
   ],
   "source": [
    "print (sum(labels))\n",
    "print (len(labels) - sum(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"text_data.pkl\",\"wb\")\n",
    "pickle.dump(wa_ma, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"test_label.pkl\",\"wb\")\n",
    "pickle.dump(labels, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
